{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import img_as_ubyte\n",
    "from pytorch3d.transforms import matrix_to_axis_angle, axis_angle_to_matrix\n",
    "# io utils\n",
    "from pytorch3d.io import load_obj\n",
    "\n",
    "# datastructures\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    BlendParams,\n",
    "    look_at_view_transform,\n",
    "    FoVPerspectiveCameras,\n",
    "    PointLights,\n",
    "    AmbientLights,\n",
    "    DirectionalLights,\n",
    "    Materials,\n",
    "    RasterizationSettings,\n",
    "    MeshRenderer,\n",
    "    MeshRasterizer,\n",
    "    SoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    SoftPhongShader,\n",
    "    TexturesVertex,\n",
    "    PerspectiveCameras\n",
    ")\n",
    "# 3D transformations functions\n",
    "from pytorch3d.transforms import Rotate, Translate\n",
    "\n",
    "# rendering components\n",
    "from pytorch3d.renderer import (\n",
    "    OpenGLPerspectiveCameras, look_at_view_transform, look_at_rotation,\n",
    "    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
    "    SoftSilhouetteShader, HardPhongShader, PointLights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "# Load the obj and ignore the textures and materials.\n",
    "verts, faces_idx, _ = load_obj(\"./teapot.obj\")\n",
    "faces = faces_idx.verts_idx\n",
    "\n",
    "# Initialize each vertex to be white in color.\n",
    "verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\n",
    "# textures = Textures(verts_rgb=verts_rgb.to(device))\n",
    "textures = TexturesVertex(verts_features=verts_rgb.to(device))\n",
    "\n",
    "# Create a Meshes object for the teapot. Here we have only one mesh in the batch.\n",
    "teapot_mesh = Meshes(\n",
    "    verts=[verts.to(device)],\n",
    "    faces=[faces.to(device)],\n",
    "    textures=textures\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an OpenGL perspective camera.\n",
    "cameras = OpenGLPerspectiveCameras(device=device)\n",
    "\n",
    "# To blend the 100 faces we set a few parameters which control the opacity and the sharpness of\n",
    "# edges. Refer to blending.py for more details.\n",
    "blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 256x256. To form the blended image we use 100 faces for each pixel. We also set bin_size and max_faces_per_bin to None which ensure that\n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for\n",
    "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of\n",
    "# the difference between naive and coarse-to-fine rasterization.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=256,\n",
    "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma,\n",
    "    faces_per_pixel=100,\n",
    ")\n",
    "\n",
    "# Create a silhouette mesh renderer by composing a rasterizer and a shader.\n",
    "silhouette_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftSilhouetteShader(blend_params=blend_params)\n",
    ")\n",
    "\n",
    "# We will also create a phong renderer. This is simpler and only needs to render one face per pixel.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=256,\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=1,\n",
    ")\n",
    "# We can add a point light in front of the object.\n",
    "lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardPhongShader(device=device, cameras=cameras, lights=lights)\n",
    ")\n",
    "\n",
    "# NEW added to generate depth image\n",
    "# using the same raster settings as the above phong renderer\n",
    "depth_renderer = MeshRasterizer(\n",
    "    cameras=cameras,\n",
    "    raster_settings=raster_settings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "### Create a reference image ###\n",
    "################################\n",
    "# Select the viewpoint using spherical angles\n",
    "distance = 3  # distance from camera to the object\n",
    "elevation = 50.0  # angle of elevation in degrees\n",
    "azimuth = 0.0  # No rotation so the camera is positioned on the +Z axis.\n",
    "\n",
    "# Get the position of the camera based on the spherical angles\n",
    "R, T = look_at_view_transform(distance, elevation, azimuth, device=device)\n",
    "\n",
    "# Render the teapot providing the values of R and T.\n",
    "silhouete = silhouette_renderer(meshes_world=teapot_mesh, R=R, T=T)\n",
    "image_ref = phong_renderer(meshes_world=teapot_mesh, R=R, T=T)\n",
    "# NEW added to generate depth image\n",
    "depth_ref = depth_renderer(meshes_world=teapot_mesh, R=R, T=T)\n",
    "depth_ref = depth_ref.zbuf\n",
    "depth_ref[depth_ref<0] = 10\n",
    "\n",
    "silhouete = silhouete.cpu().numpy()\n",
    "image_ref = image_ref.cpu().numpy()\n",
    "# NEW added to generate depth image\n",
    "depth_ref = depth_ref.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sixd2matrot(pose_6d):\n",
    "    '''\n",
    "    :param pose_6d: Nx6\n",
    "    :return: pose_matrot: Nx3x3\n",
    "    '''\n",
    "    rot_vec_1 = pose_6d[:,:3]\n",
    "    rot_vec_2 = pose_6d[:,3:6]\n",
    "    rot_vec_3 = torch.cross(rot_vec_1, rot_vec_2)\n",
    "    pose_matrot = torch.stack([rot_vec_1,rot_vec_2,rot_vec_3],dim=-1)\n",
    "    return pose_matrot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrot2sixd(pose_matrot):\n",
    "    '''\n",
    "    :param pose_matrot: Nx3x3\n",
    "    :return: pose_6d: Nx6\n",
    "    '''\n",
    "    pose_6d = torch.cat([pose_matrot[:,:3,0], pose_matrot[:,:3,1]], dim=1)\n",
    "    return pose_6d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    # def __init__(self, meshes, renderer, image_ref):\n",
    "    def __init__(self, meshes, renderer, depth_ref):\n",
    "        super().__init__()\n",
    "        self.meshes = meshes\n",
    "        self.device = meshes.device\n",
    "        self.renderer = renderer\n",
    "\n",
    "        # Get the silhouette of the reference RGB image by finding all the non zero values.\n",
    "        # image_ref = torch.from_numpy((image_ref[..., :3].max(-1) != 0).astype(np.float32))\n",
    "        # self.register_buffer('image_ref', image_ref)\n",
    "\n",
    "        # NEW added to get depth image\n",
    "        depth_ref = torch.from_numpy((depth_ref).astype(np.float32))\n",
    "        self.register_buffer('depth_ref', depth_ref)\n",
    "\n",
    "        # Create an optimizable parameter for the x, y, z position of the camera.\n",
    "        # self.camera_position = torch.from_numpy(np.array([0.0114, 2.3306, 3.0206], dtype=np.float32)).to(meshes.device)\n",
    "            # Original starting point\n",
    "            # torch.from_numpy(np.array([3.0,  6.9, +2.5], dtype=np.float32)).to(meshes.device))\n",
    "            # Set to a starting point closer to the reference depth image\n",
    "\n",
    "        self.camera_position = nn.Parameter(\n",
    "            # Original starting point\n",
    "            # torch.from_numpy(np.array([3.0,  6.9, +2.5], dtype=np.float32)).to(meshes.device))\n",
    "            # Set to a starting point closer to the reference depth image\n",
    "            torch.from_numpy(np.array([0.0114, 2.3306, 3.0206], dtype=np.float32)).to(meshes.device))\n",
    "\n",
    "\n",
    "        obj_rot = torch.FloatTensor(np.eye(3)).unsqueeze(0)\n",
    "        # obj_rot = matrix_to_axis_angle(obj_rot) #[1, 3]\n",
    "        obj_rot = matrot2sixd(obj_rot) #[1, 6]\n",
    "\n",
    "        obj_trans = torch.zeros((1, 3))\n",
    "        self.obj_rot = nn.Parameter(obj_rot.to(self.device))\n",
    "        self.obj_trans = nn.Parameter(obj_trans.to(self.device))\n",
    "\n",
    "        self.camera_position.requires_grad = False\n",
    "        self.obj_rot.requires_grad = True\n",
    "        self.obj_trans.requires_grad = True\n",
    "\n",
    "        self.verts, faces_idx, _ = load_obj(\"./teapot.obj\")\n",
    "        self.faces = faces_idx.verts_idx\n",
    "\n",
    "        # Initialize each vertex to be white in color.\n",
    "        self.verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\n",
    "        # textures = Textures(verts_rgb=verts_rgb.to(device))\n",
    "        self.textures = TexturesVertex(verts_features=verts_rgb.to(device))\n",
    "\n",
    "        self.h = torch.tensor([[0, 0, 0, 1]], dtype=torch.float32).to(self.device)\n",
    "\n",
    "    def apply_transform(self, obj_pose, obj_trans,obj_verts):\n",
    "        # obj_rot = axis_angle_to_matrix(obj_pose[:, :3]).squeeze()\n",
    "        # obj_trans = obj_pose[:, 3:].T\n",
    "        # obj_pose = torch.cat((obj_rot, obj_trans), -1)\n",
    "        # obj_mat = torch.cat([obj_pose, self.h], dim=0)\n",
    "        rot_mat = sixd2matrot(obj_pose) #[1, 3, 3]\n",
    "        obj_mat = torch.cat([rot_mat, obj_trans.unsqueeze(-1)], dim=-1) #[1, 3, 4]\n",
    "        obj_mat = torch.cat([obj_mat, self.h.unsqueeze(1)], dim=1) #[1, 4, 4]\n",
    "        obj_mat = obj_mat[0]\n",
    "\n",
    "        # Append 1 to each coordinate to convert them to homogeneous coordinates\n",
    "        h = torch.ones((obj_verts.shape[0], 1), device=self.device)\n",
    "        homogeneous_points = torch.cat((obj_verts, h), 1)\n",
    "        # Apply matrix multiplication\n",
    "        transformed_points = homogeneous_points @ obj_mat.T #[N, 4] [4, 4]\n",
    "        # Convert back to Cartesian coordinates\n",
    "        transformed_points_cartesian = transformed_points[:, :3] / transformed_points[:, 3:]\n",
    "\n",
    "        return transformed_points_cartesian\n",
    "    \n",
    "    # def apply_transform(self, obj_pose, obj_verts):\n",
    "    #     obj_rot = axis_angle_to_matrix(obj_pose[:, :3]).squeeze()\n",
    "    #     obj_trans = obj_pose[:, 3:].T\n",
    "    #     obj_pose = torch.cat((obj_rot, obj_trans), -1)\n",
    "    #     obj_mat = torch.cat([obj_pose, self.h], dim=0)\n",
    "    #     # rot_mat = sixd2matrot(obj_pose) #[1, 3, 3]\n",
    "    #     # obj_mat = torch.cat([rot_mat, obj_trans.unsqueeze(-1)], dim=-1) #[1, 3, 4]\n",
    "    #     # obj_mat = torch.cat([obj_mat, self.h.unsqueeze(1)], dim=1) #[1, 4, 4]\n",
    "    #     # obj_mat = obj_mat[0]\n",
    "\n",
    "    #     # Append 1 to each coordinate to convert them to homogeneous coordinates\n",
    "    #     h = torch.ones((obj_verts.shape[0], 1), device=self.device)\n",
    "    #     homogeneous_points = torch.cat((obj_verts, h), 1)\n",
    "    #     # Apply matrix multiplication\n",
    "    #     transformed_points = homogeneous_points @ obj_mat.T #[N, 4] [4, 4]\n",
    "    #     # Convert back to Cartesian coordinates\n",
    "    #     transformed_points_cartesian = transformed_points[:, :3] / transformed_points[:, 3:]\n",
    "\n",
    "    #     return transformed_points_cartesian\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        # Render the image using the updated camera position. Based on the new position of the\n",
    "        # camer we calculate the rotation and translation matrices\n",
    "        R = look_at_rotation(self.camera_position[None, :], device=self.device)  # (1, 3, 3)\n",
    "        T = -torch.bmm(R.transpose(1, 2), self.camera_position[None, :, None])[:, :, 0]  # (1, 3)\n",
    "\n",
    "        # obj_pose = torch.cat((self.obj_rot, self.obj_trans), -1)\n",
    "\n",
    "        init_vert = self.verts.clone().to(device)\n",
    "        # verts = self.apply_transform(obj_pose, init_vert)\n",
    "        verts = self.apply_transform(self.obj_rot, self.obj_trans, init_vert)\n",
    "        teapot_mesh = Meshes(\n",
    "            verts=[verts.to(device)],\n",
    "            faces=[self.faces.to(device)],\n",
    "            textures=self.textures\n",
    "        )\n",
    "\n",
    "        image = self.renderer(meshes_world=teapot_mesh, R=R, T=T)\n",
    "        # NEW added to generate depth image\n",
    "        image = image.zbuf\n",
    "        image[image < 0] = 10\n",
    "\n",
    "        # Calculate the silhouette loss\n",
    "        # loss = torch.sum((image[..., 3] - self.image_ref) ** 2)\n",
    "\n",
    "        # NEW Calculate the depth image loss\n",
    "        loss = torch.sum((image - self.depth_ref) ** 2)\n",
    "\n",
    "        return loss, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_output = \"./teapot_optimization_demo.gif\"\n",
    "writer = imageio.get_writer(filename_output, mode='I', duration=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(meshes=teapot_mesh, renderer=depth_renderer, depth_ref=depth_ref).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "_, image_init = model()\n",
    "plt.subplot(1, 2, 1)\n",
    "# plt.imshow(image_init.detach().squeeze().cpu().numpy()[..., 3])\n",
    "plt.imshow(image_init.detach().squeeze().cpu().numpy())  # NEW for plotting depth image\n",
    "plt.grid(False)\n",
    "plt.title(\"Starting position\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(model.depth_ref.cpu().numpy().squeeze())\n",
    "plt.grid(False)\n",
    "# plt.title(\"Reference silhouette\")\n",
    "plt.title(\"Reference depth\")  # NEW for plotting depth image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in tqdm(range(1500)):\n",
    "    optimizer.zero_grad()\n",
    "    loss, depth_current = model()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # loop.set_description('Optimizing (loss %.4f)' % loss.data)\n",
    "\n",
    "    if loss.item() < 200:\n",
    "        break\n",
    "\n",
    "    # Save outputs to create a GIF.\n",
    "    if i % 10 == 0:\n",
    "        R = look_at_rotation(model.camera_position[None, :], device=model.device)\n",
    "        T = -torch.bmm(R.transpose(1, 2), model.camera_position[None, :, None])[:, :, 0]  # (1, 3)\n",
    "        image = phong_renderer(meshes_world=model.meshes.clone(), R=R, T=T)\n",
    "        image = image[0, ..., :3].detach().squeeze().cpu().numpy()\n",
    "        image = img_as_ubyte(image)\n",
    "        writer.append_data(image)\n",
    "\n",
    "        # plt.imshow(image[..., :3])\n",
    "        plt.imshow(depth_current.detach().squeeze().cpu().numpy())\n",
    "        plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n",
    "        plt.grid(\"off\")\n",
    "        plt.axis(\"off\")\n",
    "        # plt.show()\n",
    "        plt.pause(0.01)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
